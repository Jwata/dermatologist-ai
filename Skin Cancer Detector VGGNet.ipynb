{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operations for floydhub\n",
    "# !git clone https://github.com/machrisaa/tensorflow-vgg tensorflow_vgg\n",
    "# !ln -s /data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter file already exists!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "\n",
    "vgg_dir = 'tensorflow_vgg/'\n",
    "vgg_name = 'vgg19'\n",
    "\n",
    "# Make sure vgg exists\n",
    "if not isdir(vgg_dir):\n",
    "    raise Exception(\"VGG directory doesn't exist!\")\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "vgg_param_file = '{}{}.npy'.format(vgg_dir, vgg_name)\n",
    "if not isfile(vgg_param_file):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc= vgg_name + ' Parameters') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://s3.amazonaws.com/content.udacity-data.com/nd101/{}.npy'.format(vgg_name),\n",
    "            vgg_param_file,\n",
    "            pbar.hook)\n",
    "else:\n",
    "    print(\"Parameter file already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_vgg import vgg19\n",
    "from tensorflow_vgg import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/'\n",
    "train_dir = data_dir + 'train/'\n",
    "\n",
    "classes = [d for d in os.listdir(train_dir) if os.path.isdir(train_dir + d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/junji/Development/udacity-deeplearning/dermatologist-ai/tensorflow_vgg/vgg19.npy\n",
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junji/miniconda3/envs/dermatologist-ai/lib/python3.5/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: train, class: melanoma, 10 / 374 images processed\n",
      "data: valid, class: melanoma, 10 / 30 images processed\n",
      "data: test, class: melanoma, 10 / 117 images processed\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "batch_size = 10\n",
    "batch = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    vgg = vgg19.Vgg19()\n",
    "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    \n",
    "    with tf.name_scope(\"content_vgg\"):\n",
    "        vgg.build(input_)\n",
    "    \n",
    "    \n",
    "    codes = None\n",
    "    labels = []\n",
    "\n",
    "    for d_type in ['train', 'valid', 'test']:\n",
    "        for c in classes:\n",
    "            image_dir = '{}{}/{}/'.format(data_dir, d_type, c) # e.g. data/train/melanoma/\n",
    "            files = os.listdir(image_dir)\n",
    "            for i, file in enumerate(files, 1):\n",
    "                # load image and resize it to 224x224\n",
    "                file_path = os.path.join(image_dir, file)\n",
    "                img = utils.load_image(file_path)\n",
    "                batch.append(img.reshape((1, 224, 224, 3)))\n",
    "                labels.append(c)\n",
    "\n",
    "                if i % batch_size == 0 or i == len(files):\n",
    "                    images = np.concatenate(batch)\n",
    "\n",
    "                    feed_dict = {input_: images}\n",
    "                    codes_batch = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "\n",
    "                    if codes is None:\n",
    "                        codes = codes_batch\n",
    "                    else:\n",
    "                        codes = np.concatenate((codes, codes_batch))\n",
    "\n",
    "                    batch = []\n",
    "                    print('data: {}, class: {}, {} / {} images processed'.format(d_type, c, i, len(files)))\n",
    "\n",
    "        # write codes to file\n",
    "        with open('{}_codes'.format(d_type), 'w') as f:\n",
    "            codes.tofile(f)\n",
    "            codes = None\n",
    "\n",
    "        # write labels to file\n",
    "        with open('{}_labels'.format(d_type), 'w') as f:\n",
    "            writer = csv.writer(f, delimiter='\\n')\n",
    "            writer.writerow(labels)\n",
    "            labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read codes and labels from file\n",
    "import csv\n",
    "\n",
    "# train data\n",
    "with open('train_labels') as f:\n",
    "    reader = csv.reader(f, delimiter='\\n')\n",
    "    train_labels = np.array([each for each in reader if len(each) > 0]).squeeze()\n",
    "with open('train_codes') as f:\n",
    "    train_x = np.fromfile(f, dtype=np.float32)\n",
    "    train_x = train_x.reshape((len(train_labels), -1))\n",
    "    \n",
    "# valid data\n",
    "with open('valid_labels') as f:\n",
    "    reader = csv.reader(f, delimiter='\\n')\n",
    "    valid_labels = np.array([each for each in reader if len(each) > 0]).squeeze()\n",
    "with open('valid_codes') as f:\n",
    "    val_x = np.fromfile(f, dtype=np.float32)\n",
    "    val_x = val_x.reshape((len(valid_labels), -1))\n",
    "    \n",
    "# test data\n",
    "with open('test_labels') as f:\n",
    "    reader = csv.reader(f, delimiter='\\n')\n",
    "    test_labels = np.array([each for each in reader if len(each) > 0]).squeeze()\n",
    "with open('test_codes') as f:\n",
    "    test_x = np.fromfile(f, dtype=np.float32)\n",
    "    test_x = test_x.reshape((len(test_labels), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(classes)\n",
    "\n",
    "train_y = lb.transform(train_labels)\n",
    "val_y = lb.transform(valid_labels)\n",
    "test_y = lb.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes (x, y): (10, 4096) (10, 3)\n",
      "Validation shapes (x, y): (10, 4096) (10, 3)\n",
      "Test shapes (x, y): (10, 4096) (10, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shapes (x, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation shapes (x, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test shapes (x, y):\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ = tf.placeholder(tf.float32, shape=[None, train_x.shape[1]])\n",
    "labels_ = tf.placeholder(tf.int64, shape=[None, train_y.shape[1]])\n",
    "\n",
    "fc = tf.contrib.layers.fully_connected(inputs_, 256)\n",
    "    \n",
    "logits = tf.contrib.layers.fully_connected(fc, train_y.shape[1], activation_fn=None)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels_, logits=logits)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "predicted = tf.nn.softmax(logits)\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, n_batches=10):\n",
    "    \"\"\" Return a generator that yields batches from arrays x and y. \"\"\"\n",
    "    batch_size = len(x)//n_batches\n",
    "    \n",
    "    for ii in range(0, n_batches*batch_size, batch_size):\n",
    "        # If we're not on the last batch, grab data with size batch_size\n",
    "        if ii != (n_batches-1)*batch_size:\n",
    "            X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] \n",
    "        # On the last batch, grab the rest of the data\n",
    "        else:\n",
    "            X, Y = x[ii:], y[ii:]\n",
    "        # I love generators\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: checkpoints: File exists\n",
      "Epoch: 1/10 Iteration: 0 Training loss: 3.11444\n",
      "Epoch: 1/10 Iteration: 1 Training loss: 0.00000\n",
      "Epoch: 1/10 Iteration: 2 Training loss: 0.00000\n",
      "Epoch: 1/10 Iteration: 3 Training loss: 0.00000\n",
      "Epoch: 1/10 Iteration: 4 Training loss: 0.00000\n",
      "Epoch: 0/10 Iteration: 5 Validation Acc: 1.0000\n",
      "Epoch: 1/10 Iteration: 5 Training loss: 0.00000\n",
      "Epoch: 1/10 Iteration: 6 Training loss: 0.00000\n",
      "Epoch: 1/10 Iteration: 7 Training loss: 0.00000\n",
      "Epoch: 1/10 Iteration: 8 Training loss: 0.00000\n",
      "Epoch: 1/10 Iteration: 9 Training loss: 0.00000\n",
      "Epoch: 0/10 Iteration: 10 Validation Acc: 1.0000\n",
      "Epoch: 2/10 Iteration: 10 Training loss: 0.00000\n",
      "Epoch: 2/10 Iteration: 11 Training loss: 0.00000\n",
      "Epoch: 2/10 Iteration: 12 Training loss: 0.00000\n",
      "Epoch: 2/10 Iteration: 13 Training loss: 0.00000\n",
      "Epoch: 2/10 Iteration: 14 Training loss: 0.00000\n",
      "Epoch: 1/10 Iteration: 15 Validation Acc: 1.0000\n",
      "Epoch: 2/10 Iteration: 15 Training loss: 0.00000\n",
      "Epoch: 2/10 Iteration: 16 Training loss: 0.00000\n",
      "Epoch: 2/10 Iteration: 17 Training loss: 0.00000\n",
      "Epoch: 2/10 Iteration: 18 Training loss: 0.00000\n",
      "Epoch: 2/10 Iteration: 19 Training loss: 0.00000\n",
      "Epoch: 1/10 Iteration: 20 Validation Acc: 1.0000\n",
      "Epoch: 3/10 Iteration: 20 Training loss: 0.00000\n",
      "Epoch: 3/10 Iteration: 21 Training loss: 0.00000\n",
      "Epoch: 3/10 Iteration: 22 Training loss: 0.00000\n",
      "Epoch: 3/10 Iteration: 23 Training loss: 0.00000\n",
      "Epoch: 3/10 Iteration: 24 Training loss: 0.00000\n",
      "Epoch: 2/10 Iteration: 25 Validation Acc: 1.0000\n",
      "Epoch: 3/10 Iteration: 25 Training loss: 0.00000\n",
      "Epoch: 3/10 Iteration: 26 Training loss: 0.00000\n",
      "Epoch: 3/10 Iteration: 27 Training loss: 0.00000\n",
      "Epoch: 3/10 Iteration: 28 Training loss: 0.00000\n",
      "Epoch: 3/10 Iteration: 29 Training loss: 0.00000\n",
      "Epoch: 2/10 Iteration: 30 Validation Acc: 1.0000\n",
      "Epoch: 4/10 Iteration: 30 Training loss: 0.00000\n",
      "Epoch: 4/10 Iteration: 31 Training loss: 0.00000\n",
      "Epoch: 4/10 Iteration: 32 Training loss: 0.00000\n",
      "Epoch: 4/10 Iteration: 33 Training loss: 0.00000\n",
      "Epoch: 4/10 Iteration: 34 Training loss: 0.00000\n",
      "Epoch: 3/10 Iteration: 35 Validation Acc: 1.0000\n",
      "Epoch: 4/10 Iteration: 35 Training loss: 0.00000\n",
      "Epoch: 4/10 Iteration: 36 Training loss: 0.00000\n",
      "Epoch: 4/10 Iteration: 37 Training loss: 0.00000\n",
      "Epoch: 4/10 Iteration: 38 Training loss: 0.00000\n",
      "Epoch: 4/10 Iteration: 39 Training loss: 0.00000\n",
      "Epoch: 3/10 Iteration: 40 Validation Acc: 1.0000\n",
      "Epoch: 5/10 Iteration: 40 Training loss: 0.00000\n",
      "Epoch: 5/10 Iteration: 41 Training loss: 0.00000\n",
      "Epoch: 5/10 Iteration: 42 Training loss: 0.00000\n",
      "Epoch: 5/10 Iteration: 43 Training loss: 0.00000\n",
      "Epoch: 5/10 Iteration: 44 Training loss: 0.00000\n",
      "Epoch: 4/10 Iteration: 45 Validation Acc: 1.0000\n",
      "Epoch: 5/10 Iteration: 45 Training loss: 0.00000\n",
      "Epoch: 5/10 Iteration: 46 Training loss: 0.00000\n",
      "Epoch: 5/10 Iteration: 47 Training loss: 0.00000\n",
      "Epoch: 5/10 Iteration: 48 Training loss: 0.00000\n",
      "Epoch: 5/10 Iteration: 49 Training loss: 0.00000\n",
      "Epoch: 4/10 Iteration: 50 Validation Acc: 1.0000\n",
      "Epoch: 6/10 Iteration: 50 Training loss: 0.00000\n",
      "Epoch: 6/10 Iteration: 51 Training loss: 0.00000\n",
      "Epoch: 6/10 Iteration: 52 Training loss: 0.00000\n",
      "Epoch: 6/10 Iteration: 53 Training loss: 0.00000\n",
      "Epoch: 6/10 Iteration: 54 Training loss: 0.00000\n",
      "Epoch: 5/10 Iteration: 55 Validation Acc: 1.0000\n",
      "Epoch: 6/10 Iteration: 55 Training loss: 0.00000\n",
      "Epoch: 6/10 Iteration: 56 Training loss: 0.00000\n",
      "Epoch: 6/10 Iteration: 57 Training loss: 0.00000\n",
      "Epoch: 6/10 Iteration: 58 Training loss: 0.00000\n",
      "Epoch: 6/10 Iteration: 59 Training loss: 0.00000\n",
      "Epoch: 5/10 Iteration: 60 Validation Acc: 1.0000\n",
      "Epoch: 7/10 Iteration: 60 Training loss: 0.00000\n",
      "Epoch: 7/10 Iteration: 61 Training loss: 0.00000\n",
      "Epoch: 7/10 Iteration: 62 Training loss: 0.00000\n",
      "Epoch: 7/10 Iteration: 63 Training loss: 0.00000\n",
      "Epoch: 7/10 Iteration: 64 Training loss: 0.00000\n",
      "Epoch: 6/10 Iteration: 65 Validation Acc: 1.0000\n",
      "Epoch: 7/10 Iteration: 65 Training loss: 0.00000\n",
      "Epoch: 7/10 Iteration: 66 Training loss: 0.00000\n",
      "Epoch: 7/10 Iteration: 67 Training loss: 0.00000\n",
      "Epoch: 7/10 Iteration: 68 Training loss: 0.00000\n",
      "Epoch: 7/10 Iteration: 69 Training loss: 0.00000\n",
      "Epoch: 6/10 Iteration: 70 Validation Acc: 1.0000\n",
      "Epoch: 8/10 Iteration: 70 Training loss: 0.00000\n",
      "Epoch: 8/10 Iteration: 71 Training loss: 0.00000\n",
      "Epoch: 8/10 Iteration: 72 Training loss: 0.00000\n",
      "Epoch: 8/10 Iteration: 73 Training loss: 0.00000\n",
      "Epoch: 8/10 Iteration: 74 Training loss: 0.00000\n",
      "Epoch: 7/10 Iteration: 75 Validation Acc: 1.0000\n",
      "Epoch: 8/10 Iteration: 75 Training loss: 0.00000\n",
      "Epoch: 8/10 Iteration: 76 Training loss: 0.00000\n",
      "Epoch: 8/10 Iteration: 77 Training loss: 0.00000\n",
      "Epoch: 8/10 Iteration: 78 Training loss: 0.00000\n",
      "Epoch: 8/10 Iteration: 79 Training loss: 0.00000\n",
      "Epoch: 7/10 Iteration: 80 Validation Acc: 1.0000\n",
      "Epoch: 9/10 Iteration: 80 Training loss: 0.00000\n",
      "Epoch: 9/10 Iteration: 81 Training loss: 0.00000\n",
      "Epoch: 9/10 Iteration: 82 Training loss: 0.00000\n",
      "Epoch: 9/10 Iteration: 83 Training loss: 0.00000\n",
      "Epoch: 9/10 Iteration: 84 Training loss: 0.00000\n",
      "Epoch: 8/10 Iteration: 85 Validation Acc: 1.0000\n",
      "Epoch: 9/10 Iteration: 85 Training loss: 0.00000\n",
      "Epoch: 9/10 Iteration: 86 Training loss: 0.00000\n",
      "Epoch: 9/10 Iteration: 87 Training loss: 0.00000\n",
      "Epoch: 9/10 Iteration: 88 Training loss: 0.00000\n",
      "Epoch: 9/10 Iteration: 89 Training loss: 0.00000\n",
      "Epoch: 8/10 Iteration: 90 Validation Acc: 1.0000\n",
      "Epoch: 10/10 Iteration: 90 Training loss: 0.00000\n",
      "Epoch: 10/10 Iteration: 91 Training loss: 0.00000\n",
      "Epoch: 10/10 Iteration: 92 Training loss: 0.00000\n",
      "Epoch: 10/10 Iteration: 93 Training loss: 0.00000\n",
      "Epoch: 10/10 Iteration: 94 Training loss: 0.00000\n",
      "Epoch: 9/10 Iteration: 95 Validation Acc: 1.0000\n",
      "Epoch: 10/10 Iteration: 95 Training loss: 0.00000\n",
      "Epoch: 10/10 Iteration: 96 Training loss: 0.00000\n",
      "Epoch: 10/10 Iteration: 97 Training loss: 0.00000\n",
      "Epoch: 10/10 Iteration: 98 Training loss: 0.00000\n",
      "Epoch: 10/10 Iteration: 99 Training loss: 0.00000\n",
      "Epoch: 9/10 Iteration: 100 Validation Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints\n",
    "\n",
    "epochs = 10\n",
    "iteration = 0\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for x, y in get_batches(train_x, train_y):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y}\n",
    "            loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                  \"Iteration: {}\".format(iteration),\n",
    "                  \"Training loss: {:.5f}\".format(loss))\n",
    "            iteration += 1\n",
    "            \n",
    "            if iteration % 5 == 0:\n",
    "                feed = {inputs_: val_x, labels_: val_y}\n",
    "                val_acc = sess.run(accuracy, feed_dict=feed)\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Validation Acc: {:.4f}\".format(val_acc))\n",
    "    saver.save(sess, \"checkpoints/skin_diseases.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/skin_diseases.ckpt\n",
      "Test accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: test_x,\n",
    "            labels_: test_y}\n",
    "    test_acc = sess.run(accuracy, feed_dict=feed)\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
